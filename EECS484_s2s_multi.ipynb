{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from pyfiles import graph_creation\n",
    "import time\n",
    "import holidays\n",
    "import pickle\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('../GEFCom2014 Data/GEFCom2014-E.xlsx')\n",
    "df['dow'] = df.Date.apply(lambda x: x.dayofweek)\n",
    "df['doy'] = df.Date.apply(lambda x: x.dayofyear)\n",
    "df['month'] = df.Date.apply(lambda x: x.month)\n",
    "df = df[df.load.isnull().sum():]\n",
    "df = df.reindex(columns=('doy', 'month', 'dow', 'Hour', 'T', 'load', 'Date'))\n",
    "offset = df.load.mean()\n",
    "scale = df.load.std()\n",
    "df.load -= df.load.mean()\n",
    "df.load /= df.load.std()\n",
    "df['T'] -= df['T'].mean()\n",
    "df['T'] /= df['T'].std()\n",
    "\n",
    "ush = holidays.US()\n",
    "df['is_holiday'] = df.Date.apply(lambda x: x in ush)\n",
    "\n",
    "df.month = np.cos(2*np.pi/12*df.month)\n",
    "df.Hour = np.cos(2*np.pi/24*df.Hour)\n",
    "df.dow = np.cos(2*np.pi/7*df.dow)\n",
    "\n",
    "df[['daily_load', 'daily_T']] = df[['load', 'T']].rolling(24).mean()\n",
    "df = df[23:].reset_index(drop=True)\n",
    "del df['Date'], df['doy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = df[:7*len(df)//8]\n",
    "# val = df[3*len(df)//4:7*len(df)//8]\n",
    "test = df[7*len(df)//8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def s2s_sequencify_withdaily(df, n_past: int=24, n_pred: int=24, days_past: int=21):\n",
    "    x_encoder = []  # encoder inputs\n",
    "    x_decoder = []  # decoder inputs\n",
    "    daily_encoder = []\n",
    "    targets = []\n",
    "    offset = max(n_past, days_past*24)+1\n",
    "    daily_columns = ['month', 'dow', 'Hour', 'is_holiday'] + [col for col in list(df.columns) if 'daily' in col]\n",
    "    daily_df = df[daily_columns]\n",
    "    daily_array = np.array(daily_df)\n",
    "    hourly_columns = ['Hour','month','dow','is_holiday','T','load']\n",
    "    hourly_df = df[hourly_columns]\n",
    "    x = np.array(hourly_df)\n",
    "    y = x[:,-1]\n",
    "    x = x[:,:-1]\n",
    "    for i in np.arange(offset, len(y) - n_pred):\n",
    "        x_encoder.append(np.append(x[i - n_past:i],\n",
    "                            y[i - n_past:i].reshape(-1, 1), axis=1))\n",
    "        x_decoder.append(x[i:i + n_pred])\n",
    "        daily_encoder.append(daily_array[i-24*days_past-1:i-1+24:24])\n",
    "        targets.append(y[i:i + n_pred])\n",
    "    return np.array(x_encoder), np.array(x_decoder), np.array(daily_encoder), np.array(targets)\n",
    "def batch(*vars, size=512):\n",
    "    for i in range(0, min(len(v) for v in vars), size):\n",
    "        yield (v[i:i+size] for v in vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xt, xtf, xtd, yt = s2s_sequencify_withdaily(train)\n",
    "# xv, xvf, xvd, yv = s2s_sequencify_withdaily(val)\n",
    "xtt, xttf, xttd, ytt = s2s_sequencify_withdaily(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_dim = 32\n",
    "daily_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    x = tf.placeholder(tf.float32, (None, None, xt.shape[2]), name='x_past')\n",
    "    xd = tf.placeholder(tf.float32, (None, None, xt.shape[2]), name='x_daily')\n",
    "    y = tf.placeholder(tf.float32, (None, None), name='y')\n",
    "    xf = tf.placeholder(tf.float32, (None, None, xtf.shape[2]), name='x_future')\n",
    "    \n",
    "    keep_prob = tf.placeholder_with_default(1.0, (), name='keep_prob')\n",
    "    is_training = tf.placeholder_with_default(False, (), name='is_training')\n",
    "    regularization = tf.placeholder_with_default(0.005, (), name='regularization')\n",
    "    \n",
    "    out_weight = tf.Variable(tf.random_normal((hidden_dim,))/hidden_dim, dtype=tf.float32, name='out_weight')\n",
    "    out_bias = tf.Variable(tf.zeros(1), dtype=tf.float32, name='out_bias')\n",
    "    \n",
    "    # learnable affine transformation\n",
    "    outputs  = graph_creation.other_s2s_lstm_multiresolution(x, xf, xd, hidden_dim, 2, daily_dim, use_bn=True, is_training=is_training, keep_prob=keep_prob)\n",
    "    \n",
    "    preds = tf.add(tf.einsum('ijk,k->ij', outputs, out_weight), out_bias, name='predictions')\n",
    "    \n",
    "    loss = tf.reduce_mean((y-preds)**2)\n",
    "    reg_loss = tf.nn.l2_loss(out_weight) * regularization / tf.cast(tf.shape(x)[0], tf.float32)\n",
    "    step = tf.train.AdamOptimizer().minimize(loss)\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    while True:\n",
    "        tf.get_default_session().close()\n",
    "except:\n",
    "    pass\n",
    "sess = tf.InteractiveSession(graph=g)\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(bs, keep_p=1.0):\n",
    "    perm = np.random.permutation(len(xt))\n",
    "    errors = []\n",
    "    for xs, xfs, xds, ys in batch(xt[perm], xtf[perm], xtd[perm], yt[perm], size=bs):\n",
    "        _, l= sess.run((step, loss), feed_dict={x:xs, xf:xfs, xd:xds, y:ys, keep_prob:keep_p, is_training:True})\n",
    "        errors.append(l)\n",
    "    return errors\n",
    "\n",
    "def evaluate(bs):\n",
    "    l = []\n",
    "    s = 0\n",
    "    for xs, xfs, xds, ys in batch(xtt, xttf, xttd, ytt, size=bs):\n",
    "        l.append(sess.run(loss, feed_dict={x:xs, xf:xfs, xd:xds, y:ys})*len(ys))\n",
    "        s += len(ys)\n",
    "    return sum(l)/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bs = 1024*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(bs*2) # run to make sure everything works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_errs = []\n",
    "test_errs = []\n",
    "for i in range(len(test_errs), 300):\n",
    "    start = time.time()\n",
    "    train_err = train_epoch(bs, 0.7)\n",
    "    train_errs.extend(train_err)\n",
    "    test_errs.append(evaluate(bs*2))\n",
    "    end = time.time()\n",
    "\n",
    "    if i>1 and test_errs[-1]==min(test_errs):\n",
    "        saver.save(sess, './results/mr_2L32h_bn_0.3d_24p_24f/model', global_step=len(test_errs))\n",
    "    print(f'Epoch {i} ({end-start:.2f}s): train_loss={train_errs[-1]:.4f}, test_loss={test_errs[-1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(bs=bs, kp=1.0):\n",
    "    ps = []\n",
    "    for xs, xfs, xds, ys in batch(xtt, xttf, xttd, ytt, size=bs):\n",
    "        ps.append(sess.run(preds, feed_dict={x:xs, xf:xfs, xd:xds, keep_prob:kp}))\n",
    "    return np.concatenate(ps,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = predict(bs*2)*scale + offset\n",
    "yov = ytt*scale + offset\n",
    "loss_by_horizon = ((yov-pos)**2).mean(0).astype(np.float32)\n",
    "loss_by_horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
